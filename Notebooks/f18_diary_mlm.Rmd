---
title: "f18 diary"
author: "Bryan"
date: "January 6, 2020"
output: 
  html_document:
    keep_md: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#Multilevel Modeling: A brief primer

The data are completely processed, and we've got a good idea of what's going on in them. Now, we can get the exciting part! A full overview of multilevel linear modeling (MLM) is beyond the scope of this article, but I'll give a short discussion on why this method of analysis was chosen. 

MLM is used for analyses of data that have certain groupings, as well as data with repeated observations. In the present study, there are both. We have a set of different brands, and each participant is also measured 3 times (weeks) for each brand. If we were to do a linear regressions, this structure inherently violates one of that model's assumptions: independence of errors - i.e., that errors are uncorrelaed. In repeated measures observations, we're going to have observations that are more alike than others since the same person is going to have multiple measurements! Additionally, as the EDA portion hinted at, there are going to be observations for certain brands that are more alike than others. For example, instagram will, on average, have much more favorable ratings and higher usage than Pinterest. An advantage of MLM is that the assumption of independence of errrors does not have to be true. 

In a standard linear regression, a single intercept value is estimated, which assumes a "one-size-fits-all" value. In MLM, we can estimate random intercepts for any differences in groupings. Take another look at the mean usages for each brand.

The variation in intercepts (where each brand hits the y-axis) is stark enough to illustrate that we should probably not estimate 1 intercept here. Random intercepts in MLM will be able to account for this. Similarly, we can look at participants' average usage times for all apps.

Using MLM, we will be able to estimate separate intercepts for each participant as well! By taking into account these differences, we will be able to get more accurate coefficients for our predictors since we remove differences associated with both individual participants as well as the platforms. 

The actual modeling will follow these steps:
1. Fixed intercept 
2. Random intercept - participants
3. Random intercept - brands
4. Addition of predictors

#Modeling

For the actual modeling, we'll be using the lme4 library in R. We'll also be using the performance library to easily compare models.

First, we need to import the data and necessary R packages.

```{r}
library(lme4)
#For p-values of predictors
library(lmerTest)
#Import csv
library(readr)
#VIF
library(car)

data = read_csv('~sm_final.csv') 
head(data)
```

## Fixed-intercept

We first fit a fixed intercept-only model to establish a baseline. When we compare models to see if random intercepts are justified, we need a way to quantify this. To do so, we will be doing chi-square difference tests. Each model will have an associated log-likelihood value; by subtracting our proposed model's log-likelihood from the prior model, we can perform a chi-square test to check for significant differences. With a lower log-likelihood indicating greater model fit, a significant, positive chi-square difference justifies the new model. If this is confusing, we'll set it in action in a moment.

First, we need to establish our bsaeline. 

```{r}
fixed_int <- lm(hrs_spent ~ 1, data)
summary(fixed_int)
```

## Random-intercepts - Participants

Next, we add in our first random-intercepts. 

```{r}
rand_int_parts <- lmer(hrs_spent ~ (1 | participant), REML = FALSE, data)
summary(rand_int_parts)
```

Something new we see in this output is the "Random effects" table. In the ```{r} participants``` row, we get a variance score that we can use to examine the depedence of errors discussed above. We can do this by calculating the intraclass correlation (ICC), which will give us a value that measures how much of the variation in the data is due to - in our current case - participant differences. We can manually calculate this by dividing the participant variance by the total variance: $$ICC=6.323/(6.323+39.410)=0.138$$. In other words, 13.8% of the data's variation is due to participant differences! This is not a ton, but it is not meaningless either.

To see if accounting for these differences is worthwhile, let's compare the 2 models.

```{r}
anova(rand_int_parts, fixed_int)
```

We can see that there is a reduction in log-likelihood of 184.65, and with p < 0.001, that this is significant. Our addition of random-intercepts for participants is justified.

## Random-intercepts - Platforms

Now, we'll add in random-intercepts for platforms. We will do so with participant intercepts still in the model. 

```{r}
rand_int_parts_platforms <- lmer(hrs_spent ~ (1 | platform) + (1 | participant), REML = FALSE, data)
summary(rand_int_parts_platforms)
```

We can calculate the ICC for platforms here as well: $$ICC=5.064/(5.064+6.696+34.053)=0.111$$. So 11.1% of the variation in the data is due to platform differences, and together, participant and platform differences account for 25.7% of the total variation ($$(5.064 + 6.696)/(5.064+6.696+34.053)=$$)! These findings are in line with what we gathered from the visualizations and further demonstrate the strengths of MLM. We've accounted for what would have been a significant amount of noise in a typical linear regression. 

Let's validate that random-intercepts for platforms is indeed justified.

```{r}
anova(rand_int_parts, rand_int_parts_platforms)
```

We can be comfortable with this addition, and now move onto adding our predictors. 

## Predictors

### IBIS

We have our model that now accounts for differences between platforms and participants, and we can now examine our predictors. Our first research question was if IBIS significantly predicts social media usage. To examine this, we'll model IBIS alone. 

```{r}
ibis_alone <- lmer(hrs_spent ~ percep + (1 | platform) + (1 | participant), REML = FALSE, data)
summary(ibis_alone)
```

Looking at ```{r} fixed effects``` table, we see that the coefficient for IBIS (i.e., percep) is 1.55 with, p < 0.001! We can interpret this as a 1-unit increase in IBIS (for each increasing overlap chosen in the scale), weekly social media usage for that platform increases by 1.55 hours, and that this is significant. 


### Full Model

The second research question concerned IBIS's performance relative to other popular measures. As we saw in the EDA portion, these measures do have moderate-strong correlations. We will use the variable inflation factor (VIF) to keep an eye on multicollinearity in the model. In short, VIF regresses each predictor on the others and returns a value; in general, a VIF > 10 is cause for concern. 

Since the measures are on different scales (e.g., IBIS is 1-7, use intent is 1-6), we'll also need to scale the variables. This changes the coefficient interpretations, so now instead a 1-unit increase, it will be a 1-standard deviation increase. This will allow us to directly compare the strength of each coefficient, though. Let's fit the model!

```{r}
full_model <- lmer(hrs_spent ~ scale(percep) + scale(use) + scale(op) + scale(rec) + (1 | platform) + (1 | participant), REML = FALSE, data)
summary(full_model)
```

In this complete model, only IBIS and use intention are significant. Use intention is the clear winner, with a 1-SD increase indicating 2.65 greater hours of usage, while IBIS is associated with 1.58 hours. Importantly, these coefficients are controlling for the others. For a more complete interpretation of IBIS, we can say that accounting for differences between platforms and individuals, and while holding use intention, overall opinion, and likelihood to recommend constant, a 1-SD increase in IBIS is associated with 1.58 greater hours of platform usage. 

```{r}
vif(full_model)
```

Looking at the VIF values, we can confirm that there is not a drastic amount of multicollinearity in the model either! 

# Conclusion

We did find evidence that IBIS does indeed predict social media usage. Although IBIS was outperformed by use intention, that it remained significant when controlling for use intention is impressive. Additionally, it outperformed 2 other common measures used in market research: overall opinion and likelihood to recommend.

IBIS is a single-item, easily administered scale. With preliminary evidence that it can prospectively predict social media usage, it does hold potential as an addition to studies where consumer behaviors are of interest. 
